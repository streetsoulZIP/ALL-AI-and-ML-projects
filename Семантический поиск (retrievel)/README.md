## Многоязычный семантический поиск фильмов


**Задача:** построить компактную модель семантического поиска,  
которая по пользовательскому запросу на любом из 10 языков находит релевантное описание фильма.


### Данные
- Исходные пары: **запрос → описание фильма** (только позитивные пары, английский язык).
- Перевод на **10 языков** для мультилингвального обучения.


### Подход
1. **Перевод данных** на 10 языков для многоязычного обучения. Ноутбук: nllb-used-to-tr-te-q
2. **Анализ слоёв** исходной модели для понимания важности каждого блока. Ноутбук: bge_layer_analys 
3. **Pruning модели и словаря** — сокращение количества параметров и токенов для ускорения инференса. Ноутбук: bge_m3_pruning
4. **Генерация teacher-векторов** с использованием большой модели-учителя. Ноутбук: generating-data-for-tfl-bge-m3-lw
5. **Обучение с учителем (distillation)** — перенос знаний в урезанную модель. Ноутбук: bge-tfl
6. **Файн-тюнинг под финальную задачу** с добавлением негативных пар, собранных вручную. Ноутбук: bge-fine-tune


### Модель
- Базовая архитектура: трансформер-энкодер для получения эмбеддингов запросов и описаний.
- Итоговая версия: **урезанная многоязычная модель** с поддержкой real-time инференса на CPU.


### Метрики
- Основная: **cosine similarity** между эмбеддингами (оценка качества поиска).
- Triplet loss для fine-tuning.


### Результат
- Сокращение размера модели в 3 раза (с учётом словаря) и словаря без потери качества.
- Поддержка поиска по 10 языкам в реальном времени.
- Модель умеет отличать “релевантный запрос” от “нерелевантного” (подробнее в последнем ноутбуке)
- Модель и данные для tfl/fine-tuning есть на kaggle: https://www.kaggle.com/datasets/kehhill/queries


### Дальнейшие шаги
- Расширение датасета новыми языками.
- Эксперименты с hard-negative mining.
- Интеграция в полнофункциональный поисковый сервис.
- Дообучение на шумных данных (добавить запросы ошибки, вроде пропуска букв)



