{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1yEvqosy7u3hKduyF4wHRyOY_vP4oKJUH","authorship_tag":"ABX9TyNnG92HnqsDhP7TcHZOyEK9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## **Этот ноутбук**: проводит анализ слоёв модели бге м3 прогоняя их на имеющихся данных (запросы пользователей) и ищет \"слабые слои\" - те что дублируют предыдущие. На результатах данного кода построена логика урезания модели: самые ключевые (по метрикам) оставляем, дубликаты - убираем."],"metadata":{"id":"t4DTsIPVx-a0"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jynIDqQlp4p9","executionInfo":{"status":"ok","timestamp":1757643356667,"user_tz":-180,"elapsed":150698,"user":{"displayName":"KEHHILL KEHHILL","userId":"16631532590014918715"}},"outputId":"39e12c6d-704b-4533-8edd-a1b90136b3c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved:\n","- ./bge_m3_stats_final/final_analysis.csv  (ЕДИНЫЙ CSV-отчёт)\n","- ./bge_m3_stats_final/embeddings.npz, ./bge_m3_stats_final/manifest.json\n"]}],"source":["# pip install -q transformers torch scikit-learn pandas numpy tqdm\n","\n","import os, json, math, numpy as np, pandas as pd, torch\n","from tqdm import tqdm\n","from sklearn.metrics import silhouette_score\n","from transformers import AutoTokenizer, AutoModel\n","\n","# ====== CONFIG ======\n","MODEL_NAME = \"BAAI/bge-m3\"\n","TEXT_COL, LANG_COL = \"text\", \"lang\"\n","SAMPLES_PER_LANG = 1000\n","MAX_LEN, BATCH = 36, 32         # MAX_LEN=256 обычно хватает; BATCH подгони под VRAM\n","OUT = \"./bge_m3_stats_final\"\n","os.makedirs(OUT, exist_ok=True)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.manual_seed(42); np.random.seed(42)\n","\n","# ====== LOAD YOUR DF HERE ======\n","# df = pd.read_parquet(\"/path/your_dataset.parquet\")  # должен содержать колонки text, lang\n","# assert {TEXT_COL, LANG_COL}.issubset(df.columns)\n","\n","def sample_equal(df, n):\n","    return (df.groupby(LANG_COL, group_keys=False)\n","              .apply(lambda g: g.sample(min(n, len(g)), random_state=42))\n","              .reset_index(drop=True))\n","\n","def pick_layers(total, k=None):\n","    return list(range(1, total + 1))\n","\n","\n","# ====== MODEL (FP16) ======\n","tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(\n","    MODEL_NAME,\n","    torch_dtype=torch.float16,\n","    device_map='auto'\n",").eval()\n","NUM_LAYERS = model.config.num_hidden_layers\n","SEL = pick_layers(NUM_LAYERS, k=7)  # 6 равномерно + последний\n","\n","def mean_pool(h, mask):\n","    m = mask.unsqueeze(-1).float()\n","    return (h*m).sum(1) / (m.sum(1).clamp(min=1e-6))\n","\n","def iter_batches(df, bs):\n","    for i in range(0, len(df), bs):\n","        yield df.iloc[i:i+bs]\n","def run_and_collect(df):\n","    embeds = {L: [] for L in SEL}\n","    labels = []\n","    with torch.inference_mode():\n","        for lang in df.columns:         # каждая колонка = язык\n","            texts = df[lang].dropna().astype(str).tolist()\n","            if len(texts) > SAMPLES_PER_LANG:\n","                texts = np.random.choice(texts, SAMPLES_PER_LANG, replace=False).tolist()\n","            for i in range(0, len(texts), BATCH):\n","                batch = texts[i:i+BATCH]\n","                enc = tok(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n","                out = model(**enc, output_hidden_states=True)\n","                hs = out.hidden_states\n","                am = enc[\"attention_mask\"]\n","                for L in SEL:\n","                    pooled = mean_pool(hs[L], am).float().cpu().numpy()\n","                    embeds[L].append(pooled)\n","                labels.extend([lang]*len(batch))\n","    for L in SEL:\n","        embeds[L] = np.concatenate(embeds[L], 0)\n","    return embeds, np.array(labels)\n","\n","\n","def compute_layer_metrics(embeds, labels):\n","    rows_global = []\n","    for L in SEL:\n","        X = embeds[L]\n","        norms = np.linalg.norm(X, axis=1)\n","        # cos with prev (если prev сохранён)\n","        cos_prev = np.nan\n","        if (L-1) in embeds:\n","            Xp = embeds[L-1]\n","            x = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-9)\n","            y = Xp / (np.linalg.norm(Xp, axis=1, keepdims=True)+1e-9)\n","            cos_prev = float((x*y).sum(1).mean())\n","        # silhouette\n","        try:\n","            sil = float(silhouette_score(X, labels, metric=\"cosine\"))\n","        except Exception:\n","            sil = float(\"nan\")\n","        # anisotropy ≈ mean cos over random pairs\n","        N = len(X)\n","        pairs = min(20000, N*(N-1)//2) if N>1 else 0\n","        if pairs>0:\n","            i = np.random.randint(0, N, size=pairs)\n","            j = np.random.randint(0, N, size=pairs)\n","            x = X[i]; y = X[j]\n","            x /= (np.linalg.norm(x, axis=1, keepdims=True)+1e-9)\n","            y /= (np.linalg.norm(y, axis=1, keepdims=True)+1e-9)\n","            anis = float((x*y).sum(1).mean())\n","        else:\n","            anis = float(\"nan\")\n","\n","        rows_global.append({\n","            \"scope\":\"global\",\n","            \"layer\":L,\n","            \"lang\":\"\",\n","            \"count\":int(N),\n","            \"mean_norm\":float(norms.mean()),\n","            \"std_norm\":float(norms.std()),\n","            \"cos_with_prev\":cos_prev,\n","            \"silhouette_by_lang_cosine\":sil,\n","            \"anisotropy_mean_cosine\":anis\n","        })\n","    return pd.DataFrame(rows_global)\n","\n","def compute_by_lang_metrics(embeds, labels):\n","    rows = []\n","    uniq = np.unique(labels)\n","    for L in SEL:\n","        X = embeds[L]\n","        norms = np.linalg.norm(X, axis=1)\n","        cos_prev_vec = None\n","        if (L-1) in embeds:\n","            Xp = embeds[L-1]\n","            x = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-9)\n","            y = Xp / (np.linalg.norm(Xp, axis=1, keepdims=True)+1e-9)\n","            cos_prev_vec = (x*y).sum(1)  # [N]\n","        for lg in uniq:\n","            m = (labels == lg)\n","            if not m.any(): continue\n","            rows.append({\n","                \"scope\":\"lang\",\n","                \"layer\":L,\n","                \"lang\":str(lg),\n","                \"count\":int(m.sum()),\n","                \"mean_norm\":float(norms[m].mean()),\n","                \"std_norm\":float(norms[m].std()),\n","                \"cos_with_prev\":(float(cos_prev_vec[m].mean()) if cos_prev_vec is not None else np.nan),\n","                \"silhouette_by_lang_cosine\":np.nan,\n","                \"anisotropy_mean_cosine\":np.nan\n","            })\n","    return pd.DataFrame(rows)\n","\n","def add_heuristics(df_all):\n","    # заполним NaN для косинуса нулями для нормировки\n","    t = df_all.copy()\n","    t[\"cos_with_prev_f\"] = t[\"cos_with_prev\"].fillna(0.0)\n","    def norm01(s):\n","        s = s.astype(float)\n","        lo, hi = s.min(), s.max()\n","        return (s - lo) / (hi - lo) if hi - lo > 1e-9 else s*0\n","    cos_n  = norm01(t.loc[t[\"scope\"]==\"global\",\"cos_with_prev_f\"])\n","    anis_n = norm01(t.loc[t[\"scope\"]==\"global\",\"anisotropy_mean_cosine\"].fillna(t[\"anisotropy_mean_cosine\"].min()))\n","    sil_n  = norm01(t.loc[t[\"scope\"]==\"global\",\"silhouette_by_lang_cosine\"].fillna(t[\"silhouette_by_lang_cosine\"].min()))\n","    # вставим обратно по индексам\n","    t.loc[t[\"scope\"]==\"global\",\"prune_score\"] = 0.5*cos_n + 0.3*anis_n + 0.2*sil_n\n","    # флаги (только для global)\n","    med_anis = t.loc[t[\"scope\"]==\"global\",\"anisotropy_mean_cosine\"].median()\n","    t[\"flag_prune_candidate\"]  = (t[\"scope\"]==\"global\") & (t[\"cos_with_prev_f\"]>=0.98) & (t[\"anisotropy_mean_cosine\"]>=med_anis)\n","    t[\"flag_freeze_candidate\"] = (t[\"scope\"]==\"global\") & (t[\"cos_with_prev_f\"]>=0.95) & (~t[\"flag_prune_candidate\"])\n","    return t.drop(columns=[\"cos_with_prev_f\"])\n","\n","def save_side_artifacts(embeds, labels):\n","    # embeddings.npz + manifest.json (по желанию)\n","    layers = np.array(sorted(embeds.keys()), dtype=np.int32)\n","    mats = [embeds[L] for L in layers]\n","    E = np.stack(mats, axis=0)  # [L, N, D]\n","    np.savez_compressed(f\"{OUT}/embeddings.npz\", embeddings=E, labels=labels, layers=layers)\n","    with open(f\"{OUT}/manifest.json\",\"w\") as f:\n","        json.dump({\"model\":MODEL_NAME,\"layers\":layers.tolist(),\"N\":int(E.shape[1]),\"D\":int(E.shape[2]),\n","                   \"max_len\":MAX_LEN,\"batch\":BATCH}, f, indent=2)\n","\n","def main(df):\n","    embeds, labels = run_and_collect(df)\n","    g = compute_layer_metrics(embeds, labels)         # scope=global\n","    bl = compute_by_lang_metrics(embeds, labels)      # scope=lang\n","    final = add_heuristics(pd.concat([g, bl], ignore_index=True))\n","    # порядок колонок и сохранение одного CSV\n","    cols = [\"scope\",\"layer\",\"lang\",\"count\",\"mean_norm\",\"std_norm\",\"cos_with_prev\",\n","            \"silhouette_by_lang_cosine\",\"anisotropy_mean_cosine\",\"prune_score\",\n","            \"flag_prune_candidate\",\"flag_freeze_candidate\"]\n","    final = final.reindex(columns=cols)\n","    final.sort_values([\"scope\",\"layer\",\"lang\"], inplace=True, kind=\"mergesort\")\n","    final.to_csv(f\"{OUT}/final_analysis.csv\", index=False)\n","    # вспомогательные файлы (по желанию — удобно для оффлайн-проверок)\n","    save_side_artifacts(embeds, labels)\n","    print(\"✅ Saved:\")\n","    print(f\"- {OUT}/final_analysis.csv  (ЕДИНЫЙ CSV-отчёт)\")\n","    print(f\"- {OUT}/embeddings.npz, {OUT}/manifest.json\")\n","\n","# ==== USAGE ====\n","df = pd.read_parquet(\"/content/transef_lerning_merged_10_langueges.parquet\")\n","main(df)\n"]},{"cell_type":"code","source":["analys = pd.read_csv(\"/content/bge_m3_stats_final/final_analysis.csv\")"],"metadata":{"id":"gVaPVlzPsK2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analys[analys['scope']=='global'][analys['flag_prune_candidate']==False]['layer']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"uA85XJXyqksj","executionInfo":{"status":"ok","timestamp":1757644182441,"user_tz":-180,"elapsed":35,"user":{"displayName":"KEHHILL KEHHILL","userId":"16631532590014918715"}},"outputId":"75af01f2-5a61-4fbc-a495-21736e068a7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4130465700.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  analys[analys['scope']=='global'][analys['flag_prune_candidate']==False]['layer']\n"]},{"output_type":"execute_result","data":{"text/plain":["0      1\n","1      2\n","12    13\n","14    15\n","15    16\n","16    17\n","17    18\n","18    19\n","19    20\n","20    21\n","21    22\n","22    23\n","23    24\n","Name: layer, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>layer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["# Оставляем слои: 1, 2, 5, 9, 13 (прередили \"дублирующиеся\"), 15, 17, 21, 24. Итого: 14 слоёв (урезание на ~41%). Получился классический \"бутерброд\": крайние слои важны, в середине дубликаты, что лишний раз говорит \"выбор слоёв удачный\".\n","\n","# Выходит слоёв что берём для layer-wise distillation: 13, 17, 24. Слой: 9 (теперь 4) будет подрожать слою 13, слой 17 (теперь 6) - 17, слой 24 (теперь 24) - 24.\n"],"metadata":{"id":"AHhL25UtziUK"}}]}